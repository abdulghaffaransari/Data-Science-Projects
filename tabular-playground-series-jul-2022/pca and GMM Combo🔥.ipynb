{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# import Libs","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.mixture import GaussianMixture,BayesianGaussianMixture\nfrom sklearn.preprocessing import *\nfrom sklearn.decomposition import PCA\nfrom yellowbrick.cluster import KElbowVisualizer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# import Data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-jul-2022/data.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Analysis","metadata":{}},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.drop('id',axis = 1).describe().T.style.background_gradient(cmap='Blues')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"hence there is no any column which contains null values, so we can continue our progress","metadata":{}},{"cell_type":"markdown","source":"# Data Visualization ","metadata":{}},{"cell_type":"code","source":"sns.set(rc={'figure.figsize':(25,25)})\nfor i, column in enumerate(list(train.columns), 1):\n    plt.subplot(5,6,i)\n    p=sns.histplot(x=column,data=train.sample(1000),stat='count',kde=True,color='orange')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"float_cols = train.columns[train.dtypes == 'float']\nint_cols = train.columns[train.dtypes == 'int']\nfloat_cols, int_cols","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(rc={'figure.figsize':(25,21)})\nsns.heatmap(train.corr(),annot=True,fmt='.2f')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's convenient not to use features that are correlated (hence redundant), when trying to make a proper clustering application. Thus, in this section, our main aim will be to analyse the different relationships between each of the features. Due to it, we'll start by calculating their correlation coefficients and showing them in a heatmap chart. Thus, we'll be able to determine which features are linearly related.","metadata":{}},{"cell_type":"code","source":"corr= train.loc[:,'f_00':].corr()\n# Getting the Upper Triangle of the co-relation matrix\nmatrix = np.triu(corr)\n\nfig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (22,8))\n# Heatmap without absolute values\nsns.heatmap(corr, mask=matrix, center = 0, cmap = 'vlag', ax = axes[0]).set_title('Without absolute values')\n# Heatmap with absolute values\nsns.heatmap(abs(corr), mask=matrix, center = 0, cmap = 'vlag', ax = axes[1]).set_title('With absolute values')\n\nfig.tight_layout(h_pad=1.0, w_pad=0.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scalling the Data","metadata":{}},{"cell_type":"code","source":"scaled_data = pd.DataFrame(StandardScaler().fit_transform(train.drop('id', axis = 1)))\nscaled_data.columns = train.columns[1:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Apply PCA","metadata":{}},{"cell_type":"code","source":"def apply_pca(X, transformer = False, components = -1):\n    aux = X.copy()\n    if transformer:\n        X = pd.DataFrame(transformer.fit_transform(X))\n        X.columns = aux.columns    \n    # Create principal components\n    if components == -1:\n        pca = PCA()\n    else:\n        pca = PCA(n_components = components)\n        \n    X_pca = pca.fit_transform(X)\n    # Convert to dataframe\n    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n    X_pca = pd.DataFrame(X_pca, columns=component_names)\n    # Create loadings\n    loadings = pd.DataFrame(\n        # transpose the matrix of loadings so the columns are the principal components and the rows are the original features\n        pca.components_.T,  \n        columns=component_names,\n        index=X.columns,\n    )\n    return pca, X_pca, loadings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_variance(pca, width=8, dpi=100):\n    # Create figure\n    fig, axs = plt.subplots(1, 2, figsize = (22,5))\n    n = pca.n_components_\n    grid = np.arange(1, n + 1)\n    # Explained variance\n    explainedVariance = pca.explained_variance_ratio_\n        \n    axs[0].bar(grid, explainedVariance)\n    axs[0].set(\n        xlabel=\"Component\", title=\"% Explained Variance\" \n    )\n    # Cumulative Variance\n    cv = np.cumsum(explainedVariance)\n    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n    axs[1].set(\n        xlabel=\"Component\", title=\"% Cumulative Variance\"\n    )\n    # Set up figure\n    #fig.set(figwidth=8, dpi=100)\n    fig.tight_layout(h_pad=1.0, w_pad=0.5)\n    return axs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca = PCA(n_components=3)\np = pca.fit_transform(scaled_data)\nplot_variance(pca)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Clustering","metadata":{}},{"cell_type":"code","source":"model = KMeans()\nvisualizer = KElbowVisualizer(model, k=(4,12))\n\nvisualizer.fit(scaled_data)     \nvisualizer.show() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# BGM","metadata":{}},{"cell_type":"code","source":"gmm = BayesianGaussianMixture(n_components = 7, covariance_type='full', random_state=1)\npred = gmm.fit_predict(scaled_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prop_cycle = plt.rcParams['axes.prop_cycle']\nc = [prop_cycle.by_key()['color'][i % 10] for i in pred]\n\nplt.figure(figsize=(8, 8))\nplt.scatter(p[:,0], p[:,1], s=1, label=f\"Cluster {i}\", c=c)\nplt.xlabel('PCA[0]')\nplt.ylabel('PCA[1]')\nplt.legend()\nplt.title('PCA projection')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_components = np.arange(1, 21)\nmodels = [GaussianMixture(n, covariance_type='full', random_state=0).fit(scaled_data) for n in n_components]\nplt.plot(n_components, [m.bic(scaled_data) for m in models], label='BIC')\nplt.plot(n_components, [m.aic(scaled_data) for m in models], label='AIC')\nplt.legend(loc='best')\nplt.xlabel('n_components')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"sample_submission = pd.read_csv('../input/tabular-playground-series-jul-2022/sample_submission.csv')\nsample_submission[\"Predicted\"]= pred\nsample_submission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}